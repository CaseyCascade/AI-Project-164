{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d35f38f",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38dcbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset basis\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Tools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Tokenizes sentences\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "# Models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c13dbd",
   "metadata": {},
   "source": [
    "We must first fetch the datasets that will be used in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc7ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST...\n",
      "Loading EMNIST...\n",
      "Loading IMDB_Sentiment...\n"
     ]
    }
   ],
   "source": [
    "# 1) Load MNIST\n",
    "print(\"Loading MNIST...\")\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "\n",
    "# 2) Load EMNIST\n",
    "print(\"Loading EMNIST...\")\n",
    "emnist = fetch_openml(\"EMNIST_Balanced\", version=1, as_frame=False)\n",
    "\n",
    "# 3) Load IMDB_Sentiment\n",
    "print(\"Loading IMDB_Sentiment...\")\n",
    "dataset = load_dataset(\"Kwaai/IMDB_Sentiment\", split=\"train\").shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf9faa",
   "metadata": {},
   "source": [
    "There will be 3 main models we will be using between these 3 differing datasets, those include K Nearest Neighbors, Naive Bayes, and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a518f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train, X_test, y_train, y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "    print(f\"K Nearest Neighbors' Accuracy: {accuracy_knn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d53b62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(X_train, X_test, y_train, y_test):\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    y_pred_nb = nb_classifier.predict(X_test)\n",
    "    accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "    print(f\"Naive Bayes' Accuracy: {accuracy_nb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f116babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(X_train, X_test, y_train, y_test):\n",
    "    logreg_classifier = LogisticRegression(max_iter=1000)\n",
    "    logreg_classifier.fit(X_train, y_train)\n",
    "    y_pred_logreg = logreg_classifier.predict(X_test)\n",
    "    accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "    print(f\"Logistic Regression's Accuracy: {accuracy_logreg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa19682",
   "metadata": {},
   "source": [
    "Prepare The MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbea36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Normalize MNIST Balanced\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X = X / 255.0\n",
    "\n",
    "# Optional: Apply PCA to reduce dimensionality\n",
    "USE_PCA = False\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=100)  # Try 50–150\n",
    "    X = pca.fit_transform(X)\n",
    "\n",
    "# 2) Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f383a9",
   "metadata": {},
   "source": [
    "Plug It Into Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "142dcc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputting MNIST...\n",
      "K Nearest Neighbors' Accuracy: 0.97\n",
      "Naive Bayes' Accuracy: 0.8184\n",
      "Logistic Regression's Accuracy: 0.9224\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputting MNIST...\")\n",
    "KNN(X_train, X_test, y_train, y_test)\n",
    "NB(X_train, X_test, y_train, y_test)\n",
    "LR(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d80c44",
   "metadata": {},
   "source": [
    "Prepare The EMNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06200218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Normalize EMNIST Balanced\n",
    "X, y = emnist.data, emnist.target.astype(int)\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X = X / 255.0\n",
    "\n",
    "# Optional: Apply PCA to reduce dimensionality\n",
    "USE_PCA = True\n",
    "if USE_PCA:\n",
    "    pca = PCA(n_components=100)  # Try 50–150\n",
    "    X = pca.fit_transform(X)\n",
    "\n",
    "# 2) Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e700fd3",
   "metadata": {},
   "source": [
    "Plug It Into Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbc95d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputting EMNIST...\n",
      "K Nearest Neighbors' Accuracy: 0.80\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInputting EMNIST...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m KNN(X_train, X_test, y_train, y_test)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mNB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m LR(X_train, X_test, y_train, y_test)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mNB\u001b[39m\u001b[34m(X_train, X_test, y_train, y_test)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mNB\u001b[39m(X_train, X_test, y_train, y_test):\n\u001b[32m      2\u001b[39m     nb_classifier = MultinomialNB()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mnb_classifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     y_pred_nb = nb_classifier.predict(X_test)\n\u001b[32m      5\u001b[39m     accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PiePie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PiePie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:762\u001b[39m, in \u001b[36m_BaseDiscreteNB.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    760\u001b[39m n_classes = Y.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    761\u001b[39m \u001b[38;5;28mself\u001b[39m._init_counters(n_classes, n_features)\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m alpha = \u001b[38;5;28mself\u001b[39m._check_alpha()\n\u001b[32m    764\u001b[39m \u001b[38;5;28mself\u001b[39m._update_feature_log_prob(alpha)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PiePie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:889\u001b[39m, in \u001b[36mMultinomialNB._count\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMultinomialNB (input X)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m     \u001b[38;5;28mself\u001b[39m.feature_count_ += safe_sparse_dot(Y.T, X)\n\u001b[32m    891\u001b[39m     \u001b[38;5;28mself\u001b[39m.class_count_ += Y.sum(axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PiePie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1827\u001b[39m, in \u001b[36mcheck_non_negative\u001b[39m\u001b[34m(X, whom)\u001b[39m\n\u001b[32m   1824\u001b[39m     X_min = xp.min(X)\n\u001b[32m   1826\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_min < \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1827\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhom\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Negative values in data passed to MultinomialNB (input X)."
     ]
    }
   ],
   "source": [
    "print(\"Inputting EMNIST...\")\n",
    "KNN(X_train, X_test, y_train, y_test)\n",
    "NB(X_train, X_test, y_train, y_test)\n",
    "LR(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b1802a",
   "metadata": {},
   "source": [
    "Prepare The IMDB_Sentiment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "974a688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n"
     ]
    }
   ],
   "source": [
    "# Show a sample text\n",
    "print(dataset[0][\"text\"])\n",
    "\n",
    "# Select a smaller subset for faster training\n",
    "dataset = dataset.select(range(1000))\n",
    "\n",
    "# Extract text and labels\n",
    "texts = dataset[\"text\"]\n",
    "labels = dataset[\"label\"]\n",
    "\n",
    "# TF-IDF Vectorization; Tokenizes\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "y = np.array(dataset[\"label\"])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d7b0e",
   "metadata": {},
   "source": [
    "Plug It Into Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2bb51b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputting IMBd_Sentiment...\n",
      "K Nearest Neighbors' Accuracy: 0.60\n",
      "Naive Bayes' Accuracy: 0.7350\n",
      "Logistic Regression's Accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputting IMBd_Sentiment...\")\n",
    "KNN(X_train, X_test, y_train, y_test)\n",
    "NB(X_train, X_test, y_train, y_test)\n",
    "LR(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
